{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIFA Pipeline best model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-create the best model and improve it. Does it generalize well to different train-test splits?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 27 columns):\n",
      " #   Column                    Non-Null Count  Dtype  \n",
      "---  ------                    --------------  -----  \n",
      " 0   Unnamed: 0                2000 non-null   int64  \n",
      " 1   Name                      2000 non-null   object \n",
      " 2   Age                       2000 non-null   int64  \n",
      " 3   Nationality               2000 non-null   object \n",
      " 4   Overall                   2000 non-null   int64  \n",
      " 5   Club                      1969 non-null   object \n",
      " 6   Value                     2000 non-null   object \n",
      " 7   Wage                      2000 non-null   object \n",
      " 8   Preferred Foot            1997 non-null   object \n",
      " 9   International Reputation  1997 non-null   float64\n",
      " 10  Weak Foot                 1997 non-null   float64\n",
      " 11  Skill Moves               1997 non-null   float64\n",
      " 12  Work Rate                 1997 non-null   object \n",
      " 13  Body Type                 1997 non-null   object \n",
      " 14  Position                  1997 non-null   object \n",
      " 15  Jersey Number             1997 non-null   float64\n",
      " 16  Joined                    1833 non-null   object \n",
      " 17  Loaned From               133 non-null    object \n",
      " 18  Contract Valid Until      1966 non-null   object \n",
      " 19  Height                    1997 non-null   object \n",
      " 20  Weight                    1997 non-null   object \n",
      " 21  Finishing                 1997 non-null   float64\n",
      " 22  HeadingAccuracy           1997 non-null   float64\n",
      " 23  Dribbling                 1997 non-null   float64\n",
      " 24  Reactions                 1997 non-null   float64\n",
      " 25  Vision                    1997 non-null   float64\n",
      " 26  Release Clause            1830 non-null   object \n",
      "dtypes: float64(9), int64(3), object(15)\n",
      "memory usage: 422.0+ KB\n"
     ]
    }
   ],
   "source": [
    "%run 4_pipeline_functions.ipynb\n",
    "%run 5_pipeline_clean.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Value', axis = 1)\n",
    "y = df['Value']\n",
    "\n",
    "set_features = 'num_ord_cat_features'\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_train_test_sets(X, set_features)\n",
    "\n",
    "col_trans = ColumnTransformer([\n",
    "                ('imp_num_cols', SimpleImputer(), num_features),\n",
    "                ('imp_ord_cols', SimpleImputer(), ord_features),\n",
    "                ('imp_cat_cols', SimpleImputer(fill_value = 'missing_value'), cat_features)\n",
    "            ])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "        ('scaler_num_ord', StandardScaler(), slice(0, len(num_features + ord_features))),\n",
    "        ('categoricalencoder', None, slice(len(num_features + ord_features), \\\n",
    "                                           len(num_features + ord_features + cat_features)))\n",
    "        ])\n",
    "\n",
    "steps = [('col_trans', col_trans),\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', None)]\n",
    "\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "ttr = TransformedTargetRegressor(regressor = pipeline, transformer = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 100\n",
    "\n",
    "gbr = GradientBoostingRegressor(random_state = 1)\n",
    "key = 'gradientboostingregressor'\n",
    "\n",
    "param_grid = [{'regressor__col_trans__imp_num_cols__strategy': ['most_frequent', 'mean', 'median'],\n",
    "               'regressor__col_trans__imp_ord_cols__strategy': ['most_frequent', 'mean', 'median'],\n",
    "               'regressor__col_trans__imp_cat_cols__strategy': ['most_frequent', 'constant'],\n",
    "               'regressor__preprocessor__scaler_num_ord': [None, StandardScaler(), RobustScaler()],\n",
    "               'regressor__preprocessor__categoricalencoder': [OneHotEncoder(handle_unknown = 'ignore')],\n",
    "               'regressor__preprocessor__categoricalencoder__drop': [None, 'first'],\n",
    "               'regressor__model': [gbr],\n",
    "               'regressor__model__loss': ['ls', 'lad', 'huber', 'quantile'], \n",
    "               'regressor__model__learning_rate': [0.01, 0.02, 0.05, 0.1, 1],\n",
    "               'regressor__model__n_estimators': np.arange(10, 101, 10),\n",
    "               'regressor__model__max_features': [2, 'auto', 'sqrt'],\n",
    "               'regressor__model__max_depth': [None, 3, 5, 10, 50, 100],\n",
    "               'regressor__model__min_samples_leaf': [1, 3, 5],\n",
    "               'regressor__model__min_samples_split': [2, 4, 8],\n",
    "               'transformer': [None, StandardScaler(), RobustScaler(), PowerTransformer()]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'save_pkl/best_model.pkl'\n",
    "if os.path.isfile(filename):\n",
    "    my_s = joblib.load(filename)\n",
    "else:\n",
    "    my_s = RandomizedSearchCV(ttr, param_distributions = param_grid, n_iter = n_iter, cv = 5, \\\n",
    "                                    scoring = 'neg_root_mean_squared_error', n_jobs = -1, verbose = 10, \\\n",
    "                                    random_state = 1)\n",
    "    my_s = my_s.fit(X_train, y_train)\n",
    "    joblib.dump(my_s, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'transformer': None, 'regressor__preprocessor__scaler_num_ord': RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
      "             with_scaling=True), 'regressor__preprocessor__categoricalencoder__drop': None, 'regressor__preprocessor__categoricalencoder': OneHotEncoder(categories='auto', drop=None, dtype=<class 'numpy.float64'>,\n",
      "              handle_unknown='ignore', sparse=True), 'regressor__model__n_estimators': 60, 'regressor__model__min_samples_split': 4, 'regressor__model__min_samples_leaf': 5, 'regressor__model__max_features': 'auto', 'regressor__model__max_depth': None, 'regressor__model__loss': 'ls', 'regressor__model__learning_rate': 0.1, 'regressor__model': GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n",
      "                          init=None, learning_rate=0.1, loss='ls',\n",
      "                          max_depth=None, max_features='auto',\n",
      "                          max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
      "                          min_impurity_split=None, min_samples_leaf=5,\n",
      "                          min_samples_split=4, min_weight_fraction_leaf=0.0,\n",
      "                          n_estimators=60, n_iter_no_change=None,\n",
      "                          presort='deprecated', random_state=1, subsample=1.0,\n",
      "                          tol=0.0001, validation_fraction=0.1, verbose=0,\n",
      "                          warm_start=False), 'regressor__col_trans__imp_ord_cols__strategy': 'most_frequent', 'regressor__col_trans__imp_num_cols__strategy': 'most_frequent', 'regressor__col_trans__imp_cat_cols__strategy': 'constant'}\n",
      "Best CV score: 8.556513E+05\n",
      "Test score: 3.663789E+05\n"
     ]
    }
   ],
   "source": [
    "best_est = my_s.best_estimator_\n",
    "best_est.fit(X_train, y_train)\n",
    "y_pred = best_est.predict(X_test)\n",
    "test_score = format(np.sqrt(mean_squared_error(y_test, y_pred)), 'E')\n",
    "best_CV_score = format(-my_s.best_score_, 'E')\n",
    "print(my_s.best_params_)\n",
    "print('Best CV score:', best_CV_score)\n",
    "print('Test score:', test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Investigate how well the model generalizes to other train-test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV score: 7.385445E+05\n",
      "Test score: 5.368910E+05\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = get_train_test_sets(X, set_features, random_state = 2)\n",
    "best_est.fit(X_train, y_train)\n",
    "y_pred = best_est.predict(X_test)\n",
    "test_score = format(np.sqrt(mean_squared_error(y_test, y_pred)), 'E')\n",
    "best_CV_score = format(-my_s.best_score_, 'E')\n",
    "#print(my_s.best_params_)\n",
    "print('Best CV score:', best_CV_score)\n",
    "print('Test score:', test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There might be a risk of overfitting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
